<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Graph Attention Networks - From Text to Graph</title>
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
    <style>
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 20px;
            background: #f8f9fa;
            line-height: 1.6;
        }
        
        .section {
            background: white;
            margin: 20px 0;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .input-box {
            width: 100%;
            padding: 15px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            resize: vertical;
        }
        
        .process-btn {
            background: #4285f4;
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 6px;
            cursor: pointer;
            margin: 15px 0;
        }
        
        .ascii-section {
            background: #f8f9fa;
            padding: 20px;
            border-left: 4px solid #4285f4;
            margin: 20px 0;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }
        
        .viz-container {
            width: 100%;
            text-align: center;
            margin: 30px 0;
        }
        
        .stage-title {
            font-size: 1.5em;
            color: #2c3e50;
            margin: 30px 0 15px 0;
            text-align: center;
        }
        
        svg {
            border: 1px solid #dee2e6;
            background: white;
            margin: 10px 0;
        }
        
        .token-badge {
            display: inline-block;
            background: #4285f4;
            color: white;
            padding: 8px 15px;
            margin: 5px;
            border-radius: 20px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="section">
        <h1>üß† Graph Attention Networks: From Text to Graph</h1>
        <p>Enter text below to see how Graph Attention Networks process sentences step by step:</p>
        
        <textarea id="input-text" placeholder="Enter your paragraph here..." maxlength="100" rows="2" style="width: 100%; padding: 10px; font-size: 16px; border: 2px solid #ddd; border-radius: 6px;">Neural networks learn patterns efficiently. Graph attention mechanisms capture relationships.</textarea>
        <button onclick="processText()" style="background: #4285f4; color: white; border: none; padding: 10px 20px; margin: 10px 0; border-radius: 6px; cursor: pointer;">üöÄ Process with GAT</button>
    </div>

    <!-- Step 1: ASCII Tokenization -->
    <div class="section">
        <h2 class="stage-title">üìù Step 1: Tokenization</h2>
        <div id="tokenization-ascii" class="ascii-section">
Click "Process with GAT" to see tokenization...
        </div>
        <div id="tokenization-viz" class="viz-container"></div>
    </div>

    <!-- Step 2: ASCII Embeddings -->
    <div class="section">
        <h2 class="stage-title">üéØ Step 2: Word Embeddings</h2>
        <div id="embedding-ascii" class="ascii-section">
Processing will show embedding vectors here...
        </div>
        <div id="embedding-viz" class="viz-container"></div>
    </div>

    <!-- Step 3: ASCII Attention -->
    <div class="section">
        <h2 class="stage-title">‚ö° Step 3: Attention Computation</h2>
        <div id="attention-ascii" class="ascii-section">
Attention matrix will appear here...
        </div>
        <div id="attention-viz" class="viz-container"></div>
    </div>

    <!-- Step 4: ASCII Aggregation -->
    <div class="section">
        <h2 class="stage-title">üîÑ Step 4: Message Aggregation</h2>
        <div id="aggregation-ascii" class="ascii-section">
Message passing explanation will show here...
        </div>
        <div id="aggregation-viz" class="viz-container"></div>
    </div>

    <!-- Step 5: Final Graphs -->
    <div class="section">
        <h2 class="stage-title">üéØ Step 5: Final Graph Representations</h2>
        <div id="final-ascii" class="ascii-section">
Final graph comparison will appear here...
        </div>
        <div id="final-viz" class="viz-container"></div>
    </div>

    <script>
        let currentTokens = [];
        let currentEmbeddings = [];
        let attentionMatrix = [];

        async function processText() {
            const text = document.getElementById('input-text').value.trim();
            if (!text) return;

            console.log('üöÄ Processing:', text);

            // Step 1: Tokenization
            await processTokenization(text);
            
            // Step 2: Embeddings  
            await processEmbeddings();
            
            // Step 3: Attention
            await processAttention();
            
            // Step 4: Aggregation
            await processAggregation();
            
            // Step 5: Final graphs
            await processFinalGraphs();
        }

        async function processTokenization(text) {
            // Split into sentences and tokens
            const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
            const tokens = [];
            
            sentences.forEach((sentence, sentIdx) => {
                const words = sentence.trim().split(/\s+/).filter(w => w.length > 0);
                words.forEach((word, wordIdx) => {
                    tokens.push({
                        word: word,
                        globalIndex: tokens.length,
                        sentenceIndex: sentIdx,
                        wordIndex: wordIdx
                    });
                });
            });
            
            currentTokens = tokens;

            // ASCII explanation
            const ascii = `INPUT: "${text}"
       ‚Üì (sentence splitting)
SENTENCES: ${sentences.map((s, i) => `[${i}] "${s.trim()}"`).join('\n           ')}
           ‚Üì (word tokenization)
TOKENS: ${JSON.stringify(tokens.map(t => t.word))}
        ‚Üì (index assignment)
INDICES: [${tokens.map(t => t.globalIndex).join(', ')}]

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TOKEN TO INDEX MAPPING                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
${tokens.map(t => `‚îÇ ${t.word.padEnd(15)} ‚Üí ${t.globalIndex.toString().padStart(2)} (sentence ${t.sentenceIndex})${' '.repeat(20)}‚îÇ`).join('\n')}
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`;

            document.getElementById('tokenization-ascii').textContent = ascii;

            // Visualization
            const container = document.getElementById('tokenization-viz');
            container.innerHTML = '';
            
            tokens.forEach((token, i) => {
                const badge = document.createElement('div');
                badge.className = 'token-badge';
                badge.textContent = `${i}: ${token.word}`;
                container.appendChild(badge);
            });
        }

        async function processEmbeddings() {
            // Generate simulated embeddings
            currentEmbeddings = currentTokens.map(token => 
                Array.from({length: 8}, () => (Math.random() - 0.5) * 2) // 8D for display
            );

            // ASCII explanation
            const ascii = `WORD EMBEDDINGS (8-dimensional for display):

${currentTokens.map((token, i) => 
    `${token.word.padEnd(12)} ‚Üí [${currentEmbeddings[i].map(x => x.toFixed(2).padStart(6)).join(', ')}]`
).join('\n')}

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EMBEDDING MATRIX H                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         dim‚ÇÄ   dim‚ÇÅ   dim‚ÇÇ   dim‚ÇÉ   dim‚ÇÑ   dim‚ÇÖ   dim‚ÇÜ   dim‚Çá‚îÇ
${currentTokens.map((token, i) => 
    `‚îÇ ${token.word.padEnd(8)}‚îÇ${currentEmbeddings[i].map(x => x.toFixed(1).padStart(6)).join('  ')} ‚îÇ`
).join('\n')}
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Each word is represented as a dense vector capturing semantic meaning.`;

            document.getElementById('embedding-ascii').textContent = ascii;

            // Visualization - 2D embedding plot
            const container = document.getElementById('embedding-viz');
            container.innerHTML = '';
            
            const svg = d3.select(container).append('svg')
                .attr('width', window.innerWidth - 100)
                .attr('height', 400);
            
            const width = parseInt(svg.attr('width'));
            const height = parseInt(svg.attr('height'));
            const margin = 60;
            
            // Plot embeddings (using first 2 dimensions)
            currentTokens.forEach((token, i) => {
                const x = margin + (currentEmbeddings[i][0] + 2) * (width - 2*margin) / 4;
                const y = margin + (2 - currentEmbeddings[i][1]) * (height - 2*margin) / 4;
                
                svg.append('circle')
                    .attr('cx', x)
                    .attr('cy', y)
                    .attr('r', 8)
                    .attr('fill', `hsl(${i * 360 / currentTokens.length}, 70%, 60%)`);
                
                svg.append('text')
                    .attr('x', x)
                    .attr('y', y - 15)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .text(token.word);
            });
        }

        async function processAttention() {
            const n = currentTokens.length;
            attentionMatrix = Array(n).fill().map(() => Array(n).fill(0));
            
            // Compute attention weights
            for (let i = 0; i < n; i++) {
                let rowSum = 0;
                for (let j = 0; j < n; j++) {
                    if (i !== j) {
                        const distance = Math.abs(i - j);
                        const semantic = simulateSemanticSimilarity(currentTokens[i].word, currentTokens[j].word);
                        const score = Math.exp(-distance * 0.3) * (0.5 + semantic);
                        attentionMatrix[i][j] = score;
                        rowSum += score;
                    }
                }
                // Normalize
                for (let j = 0; j < n; j++) {
                    attentionMatrix[i][j] = attentionMatrix[i][j] / rowSum;
                }
            }

            // ASCII explanation
            const ascii = `ATTENTION MECHANISM:
For each word pair (i,j), compute attention score:

e_ij = LeakyReLU(a^T [W¬∑h_i || W¬∑h_j])
Œ±_ij = softmax(e_ij) = exp(e_ij) / Œ£‚Çñ exp(e_ik)

ATTENTION MATRIX (${n}√ó${n}):
         ${currentTokens.map(t => t.word.substring(0,8).padStart(8)).join(' ')}
       ‚îå${'‚îÄ'.repeat(n * 9 + 1)}‚îê
${attentionMatrix.map((row, i) => 
    `${currentTokens[i].word.padEnd(8)}‚îÇ${row.map(val => val.toFixed(2).padStart(8)).join(' ')} ‚îÇ`
).join('\n')}
       ‚îî${'‚îÄ'.repeat(n * 9 + 1)}‚îò

Each row sums to 1.0 (softmax normalized)
High values = strong attention between words`;

            document.getElementById('attention-ascii').textContent = ascii;

            // Visualization - attention heatmap
            const container = document.getElementById('attention-viz');
            container.innerHTML = '';
            
            const svg = d3.select(container).append('svg')
                .attr('width', window.innerWidth - 100)
                .attr('height', Math.max(400, n * 40 + 100));
            
            const width = parseInt(svg.attr('width'));
            const cellSize = Math.min(40, (width - 200) / n);
            const startX = (width - n * cellSize) / 2;
            const startY = 80;
            
            // Draw attention matrix
            for (let i = 0; i < n; i++) {
                for (let j = 0; j < n; j++) {
                    const weight = attentionMatrix[i][j];
                    const x = startX + j * cellSize;
                    const y = startY + i * cellSize;
                    
                    svg.append('rect')
                        .attr('x', x)
                        .attr('y', y)
                        .attr('width', cellSize)
                        .attr('height', cellSize)
                        .attr('fill', `rgba(66, 133, 244, ${weight})`)
                        .attr('stroke', '#333')
                        .attr('stroke-width', 0.5);
                    
                    if (weight > 0.15) {
                        svg.append('text')
                            .attr('x', x + cellSize/2)
                            .attr('y', y + cellSize/2 + 4)
                            .attr('text-anchor', 'middle')
                            .attr('font-size', '10px')
                            .attr('fill', weight > 0.5 ? 'white' : 'black')
                            .text(weight.toFixed(2));
                    }
                }
            }
            
            // Labels
            currentTokens.forEach((token, i) => {
                // Top labels
                svg.append('text')
                    .attr('x', startX + i * cellSize + cellSize/2)
                    .attr('y', startY - 10)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .text(token.word);
                
                // Left labels
                svg.append('text')
                    .attr('x', startX - 10)
                    .attr('y', startY + i * cellSize + cellSize/2 + 4)
                    .attr('text-anchor', 'end')
                    .attr('font-size', '12px')
                    .text(token.word);
            });
        }

        async function processAggregation() {
            // ASCII explanation
            const ascii = `MESSAGE PASSING & AGGREGATION:
h'_i = œÉ(Œ£‚±º Œ±_ij ¬∑ W ¬∑ h‚±º)

For each word, aggregate neighbor information weighted by attention:

${currentTokens.map((token, i) => {
    const topConnections = attentionMatrix[i]
        .map((weight, j) => ({ weight, word: currentTokens[j].word, index: j }))
        .filter(conn => conn.weight > 0.1 && conn.index !== i)
        .sort((a, b) => b.weight - a.weight)
        .slice(0, 3);
    
    return `${token.word.padEnd(12)} pays attention to:
${topConnections.map(conn => `  ‚Ä¢ ${conn.word} (${conn.weight.toFixed(2)})`).join('\n')}`;
}).join('\n\n')}

AGGREGATED FEATURES:
Each word now has updated representation incorporating neighbor information.`;

            document.getElementById('aggregation-ascii').textContent = ascii;

            // Visualization - message passing
            const container = document.getElementById('aggregation-viz');
            container.innerHTML = '';
            
            const svg = d3.select(container).append('svg')
                .attr('width', window.innerWidth - 100)
                .attr('height', 300);
            
            const width = parseInt(svg.attr('width'));
            const nodeSpacing = (width - 100) / (currentTokens.length + 1);
            const y = 150;
            
            // Draw nodes and attention flows
            currentTokens.forEach((token, i) => {
                const x = 50 + (i + 1) * nodeSpacing;
                
                // Node
                svg.append('circle')
                    .attr('cx', x)
                    .attr('cy', y)
                    .attr('r', 25)
                    .attr('fill', `hsl(${i * 360 / currentTokens.length}, 70%, 60%)`)
                    .attr('stroke', '#333')
                    .attr('stroke-width', 2);
                
                // Label
                svg.append('text')
                    .attr('x', x)
                    .attr('y', y + 4)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '11px')
                    .attr('font-weight', 'bold')
                    .text(token.word.length > 8 ? token.word.substring(0, 6) + '...' : token.word);
                
                // Draw top attention connections
                currentTokens.forEach((targetToken, j) => {
                    if (i !== j && attentionMatrix[i][j] > 0.2) {
                        const targetX = 50 + (j + 1) * nodeSpacing;
                        const weight = attentionMatrix[i][j];
                        
                        // Curved attention line
                        const midY = y - 60;
                        svg.append('path')
                            .attr('d', `M ${x} ${y-25} Q ${(x+targetX)/2} ${midY} ${targetX} ${y-25}`)
                            .attr('stroke', '#4285f4')
                            .attr('stroke-width', weight * 5)
                            .attr('stroke-opacity', weight)
                            .attr('fill', 'none');
                    }
                });
            });
        }

        async function processFinalGraphs() {
            // ASCII comparison
            const n = currentTokens.length;
            const ascii = `FINAL GRAPH COMPARISON:

1. SEQUENTIAL GRAPH (Current Implementation):
   ${currentTokens.map(t => t.word).join(' ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ')}
   
   Adjacency Matrix (Sequential):
         ${currentTokens.map(t => t.word.substring(0,4).padStart(4)).join(' ')}
       ‚îå${'‚îÄ'.repeat(n * 5 + 1)}‚îê
${Array(n).fill().map((_, i) => 
    `${currentTokens[i].word.padEnd(8)}‚îÇ${Array(n).fill().map((_, j) => (j === i + 1 ? ' 1  ' : ' 0  ')).join('')} ‚îÇ`
).join('\n')}
       ‚îî${'‚îÄ'.repeat(n * 5 + 1)}‚îò

2. GAT GRAPH (Attention-Based):
   All words connected with learned attention weights
   
   Key Attention Connections:
${currentTokens.map((token, i) => {
    const topConn = attentionMatrix[i]
        .map((w, j) => ({ weight: w, target: currentTokens[j].word, index: j }))
        .filter(c => c.weight > 0.3 && c.index !== i)
        .sort((a, b) => b.weight - a.weight)[0];
    
    return topConn ? `   ${token.word} ‚îÄ‚îÄ${topConn.weight.toFixed(2)}‚îÄ‚îÄ‚Üí ${topConn.target}` : `   ${token.word} (no strong connections)`;
}).join('\n')}`;

            document.getElementById('final-ascii').textContent = ascii;

            // Visualization - side by side comparison
            const container = document.getElementById('final-viz');
            container.innerHTML = '<h3>Sequential Graph</h3><div id="seq-graph"></div><h3>GAT Graph</h3><div id="gat-graph"></div>';
            
            // Sequential graph
            const seqSvg = d3.select('#seq-graph').append('svg')
                .attr('width', window.innerWidth - 100)
                .attr('height', 200);
            
            const seqWidth = parseInt(seqSvg.attr('width'));
            const seqSpacing = (seqWidth - 100) / (currentTokens.length + 1);
            
            currentTokens.forEach((token, i) => {
                const x = 50 + (i + 1) * seqSpacing;
                const y = 100;
                
                // Rectangle
                seqSvg.append('rect')
                    .attr('x', x - 40)
                    .attr('y', y - 20)
                    .attr('width', 80)
                    .attr('height', 40)
                    .attr('fill', '#ffd700')
                    .attr('stroke', '#333')
                    .attr('rx', 8);
                
                // Text
                seqSvg.append('text')
                    .attr('x', x)
                    .attr('y', y + 5)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '12px')
                    .text(token.word.length > 10 ? token.word.substring(0, 8) + '..' : token.word);
                
                // Arrow
                if (i < currentTokens.length - 1) {
                    const nextX = 50 + (i + 2) * seqSpacing;
                    seqSvg.append('path')
                        .attr('d', `M ${x + 40} ${y} L ${nextX - 40} ${y}`)
                        .attr('stroke', '#333')
                        .attr('stroke-width', 2)
                        .attr('marker-end', 'url(#arrow)');
                }
            });
            
            // Add arrow marker
            seqSvg.append('defs').append('marker')
                .attr('id', 'arrow')
                .attr('markerWidth', 10)
                .attr('markerHeight', 10)
                .attr('refX', 8)
                .attr('refY', 3)
                .attr('orient', 'auto')
                .append('path')
                .attr('d', 'M0,0 L0,6 L9,3 z')
                .attr('fill', '#333');

            // GAT graph
            const gatSvg = d3.select('#gat-graph').append('svg')
                .attr('width', window.innerWidth - 100)
                .attr('height', 250);
            
            const gatWidth = parseInt(gatSvg.attr('width'));
            
            currentTokens.forEach((token, i) => {
                const x = 50 + (i + 1) * seqSpacing;
                const y = 125;
                
                // Node
                gatSvg.append('circle')
                    .attr('cx', x)
                    .attr('cy', y)
                    .attr('r', 20)
                    .attr('fill', `hsl(${i * 360 / currentTokens.length}, 70%, 60%)`)
                    .attr('stroke', '#333')
                    .attr('stroke-width', 2);
                
                // Label
                gatSvg.append('text')
                    .attr('x', x)
                    .attr('y', y + 4)
                    .attr('text-anchor', 'middle')
                    .attr('font-size', '10px')
                    .attr('font-weight', 'bold')
                    .text(token.word.substring(0, 6));
                
                // Attention connections
                currentTokens.forEach((target, j) => {
                    if (i !== j && attentionMatrix[i][j] > 0.25) {
                        const targetX = 50 + (j + 1) * seqSpacing;
                        const weight = attentionMatrix[i][j];
                        
                        gatSvg.append('path')
                            .attr('d', `M ${x} ${y-20} Q ${(x+targetX)/2} ${y-80} ${targetX} ${y-20}`)
                            .attr('stroke', '#4285f4')
                            .attr('stroke-width', weight * 4)
                            .attr('stroke-opacity', weight * 0.8)
                            .attr('fill', 'none');
                    }
                });
            });
        }

        function simulateSemanticSimilarity(word1, word2) {
            const groups = {
                'neural': ['networks', 'learning', 'algorithms'],
                'attention': ['mechanisms', 'capture', 'relationships'], 
                'models': ['transform', 'language', 'processing']
            };
            
            for (const [key, group] of Object.entries(groups)) {
                if ((group.includes(word1.toLowerCase()) || word1.toLowerCase() === key) &&
                    (group.includes(word2.toLowerCase()) || word2.toLowerCase() === key)) {
                    return 0.8;
                }
            }
            return Math.random() * 0.2;
        }
    </script>
</body>
</html>