# ğŸ§  TextGraph - GAT Attention Visualization

ğŸš€ Interactive text-to-graph visualization with Graph Attention Networks (GAT) computation and transparency-based attention mapping.

![TextGraph GAT Visualization](https://raw.githubusercontent.com/1kaiser/TextGraph/master/screenshots/gat-test-worker-2-1756616821922.png)

> ğŸ¯ **Graph Attention Networks in action**: Interactive visualization showing "Attention mechanisms allow models" with computed attention weights displayed in the adjacency matrix (0.0-0.9 range) and corresponding node transparency. The dual-input system allows context paragraph analysis with targeted query sentences.

## ğŸ¯ Overview

This system implements a complete GAT attention visualization where:
- ğŸ“„ **Paragraph context** provides attention computation base
- ğŸ¯ **Query sentences** are analyzed within paragraph context  
- ğŸ”— **Graph nodes** get transparency based on attention strength
- ğŸ“Š **Adjacency matrix** shows attention values with normalized transparency
- ğŸ–±ï¸ **Hover interactions** highlight related elements between graph and matrix

## ğŸš€ Quick Start

### âš¡ One-Line Server Start
```bash
./start-server.sh          # ğŸ¨ Main TextGraph visualization
./start-gat-server.sh      # ğŸ§  GAT attention visualization
```

### ğŸ”§ Manual Start
```bash
npm run start              # ğŸ¨ Main app: localhost:3000 + local IP
npm run start:gat          # ğŸ§  GAT app: localhost:42040 + local IP
```

**ğŸŒ Access:**
- ğŸ  **Local**: http://localhost:3000 (main) | http://localhost:42040 (GAT)  
- ğŸŒ **Network**: http://[YOUR_LOCAL_IP]:3000 | http://[YOUR_LOCAL_IP]:42040

## ğŸ“± Interface

### ğŸ–Šï¸ Input System
- ğŸ“„ **Context Paragraph** (collapsible) - Full paragraph for attention context
- ğŸ¯ **Query Sentence** - Target sentence to analyze  
- ğŸ”„ **Compute GAT** - Trigger attention computation and visualization

### ğŸ¨ Visual Elements
- ğŸ”— **Graph nodes** - Words with transparency = attention strength
- ğŸ“Š **Adjacency matrix** - Attention values with color-coded transparency
- ğŸ–±ï¸ **Hover interactions** - Graphâ†”Matrix highlighting
- ğŸƒ **Draggable components** - Both input box and graph are draggable

## ğŸ§  GAT Implementation

### âš™ï¸ Core Functions

#### `computeGATAttention(paragraphText, queryText)`
**ğŸ“¥ Inputs:**
- `paragraphText`: Full context paragraph (string)
- `queryText`: Target sentence to analyze (string)

**ğŸ“¤ Returns:**
```javascript
{
  queryTokens: ["graph", "attention", "mechanisms"],
  attentionMatrix: [[0.003, 0.047, 0.950], [0.531, 0.031, 0.439], [0.958, 0.039, 0.003]],
  minAttention: 0.0027,
  maxAttention: 0.9580,
  computationDetails: {embeddings: 3, matrixSize: "3x3", nonZeroElements: 6}
}
```

### ğŸ§® Mathematical Pipeline

1. ğŸ”¢ **`createEmbeddingMatrix()`** - Generate 64-dimensional deterministic embeddings
2. ğŸ¯ **`createAttentionScoreMatrix()`** - Compute dot products + LeakyReLU activation  
3. ğŸ“Š **`applySoftmaxNormalization()`** - Row-wise probability normalization

### ğŸ¨ Visualization Pipeline

1. ğŸŒˆ **`applyAttentionColoring()`** - Apply transparency to graph nodes
2. ğŸ”„ **`updateExistingMatrix()`** - Update original adjacency matrix SVG
3. ğŸ­ **`updateOriginalMatrixWithAttention()`** - Apply attention to existing rectangles
4. ğŸ”¢ **`addAttentionScoresToMatrix()`** - Overlay numerical attention scores
5. ğŸ–±ï¸ **`setupAttentionHoverInteractions()`** - Enable graphâ†”matrix highlighting

## ğŸ¨ Transparency Mapping

### ğŸ”— Graph Nodes
- ğŸŒŸ **Opacity = attention strength** (0â†’100% transparent, 1â†’100% opaque)
- ğŸ¨ **Fixed colors**: Blue text, yellow rectangles

### ğŸ“Š Adjacency Matrix  
- ğŸ”§ **Transparency normalized** by min/max across full matrix
- â¬†ï¸ **High attention** = low transparency (more opaque)
- ğŸ”¢ **Attention values** displayed as text overlays (0.x format)
- ğŸ“ **Matrix squares**: 1.3x larger for better readability
- â¡ï¸ **Arrow routing**: Straight arrows for clean connections
- ğŸ¯ **Color coding**: Blue for connections, gray for self-attention

## ğŸ¯ Hover Interactions

### ğŸ–±ï¸ Graph Node Hover
- ğŸ” Highlights corresponding **matrix row and column**
- ğŸ“Š Shows attention weights in console tooltip

### ğŸ“Š Matrix Cell Hover  
- ğŸ” Highlights corresponding **graph nodes**
- ğŸ’ª Displays connection strength and interpretation

## ğŸ“Š Attention Computation Details

```
ğŸ“‹ WORKFLOW:
ğŸ“„ Paragraph â†’ ğŸ”¤ Tokenization â†’ ğŸ”¢ Embeddings â†’ ğŸ¯ Attention Scores â†’ ğŸ“Š Softmax â†’ ğŸ¨ Visualization

ğŸ’¡ EXAMPLE:
ğŸ“¥ Input: "Neural networks are powerful. Graph attention mechanisms capture relationships."
ğŸ¯ Query: "Graph attention mechanisms"

ğŸ”¤ Tokens: [graph, attention, mechanisms]
ğŸ“Š Matrix: 3x3 attention weights
ğŸ“ˆ Range: [0.0027, 0.9580]
ğŸ¨ Result: Transparency-mapped visualization
```

## ğŸ”§ Technical Stack

- ğŸ§® **MathJax**: Mathematical attention computation
- ğŸ¨ **D3.js**: SVG-based matrix and graph rendering  
- ğŸš€ **JavaScript ES6**: Module-based architecture
- ğŸ“¦ **Parcel**: Development server with hot reload
- ğŸ­ **Playwright**: Multi-worker testing and validation

## ğŸ“š References

### ğŸ“„ Core Papers & Publications
- ğŸ§  **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** - Vaswani et al. (2017) - Foundational transformer attention mechanism
- ğŸ”— **[Graph Attention Networks](https://arxiv.org/abs/1710.10903)** - VeliÄkoviÄ‡ et al. (2018) - GAT architecture and attention computation
- ğŸ¨ **[Text as Graph](https://distill.pub/2021/gnn-intro/)** - Google Distill (2021) - Interactive visualization inspiration

### ğŸ› ï¸ Implementation References  
- ğŸ¨ **[D3.js v5 Documentation](https://d3js.org/)** - SVG visualization and DOM manipulation
- ğŸ“¦ **[Parcel Bundler](https://parceljs.org/)** - Zero-configuration build system
- ğŸ§® **[MathJax](https://www.mathjax.org/)** - Mathematical notation rendering
- ğŸ­ **[Playwright Testing](https://playwright.dev/)** - Automated browser testing framework

### ğŸ§® Mathematical Background
- ğŸ“Š **Softmax Normalization**: Row-wise probability distribution for attention weights
- âš¡ **LeakyReLU Activation**: `f(x) = max(0.01x, x)` for attention score computation
- ğŸ¯ **Dot Product Attention**: `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`
- ğŸ“ˆ **Graph Adjacency Matrices**: Representing word relationships as mathematical structures

### ğŸš€ Development Resources
- ğŸŒ **Local Network Access**: Automatic IP detection with `ip route get 1`
- ğŸ’» **Cross-Platform Compatibility**: Bash scripts with fallback IP resolution
- ğŸ”¥ **Hot Reload Development**: Parcel dev server with `--no-hmr` for stability

---

âœ¨ *This implementation provides a complete educational tool for understanding Graph Attention Networks through interactive visualization of attention weights, mathematical computation transparency, and real-time exploration of attention patterns.* ğŸ“