<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Graph Attention Networks Explorer</title>
    <link rel="stylesheet" href="gat-explorer.css">
    
    <!-- TensorFlow.js for neural network computations -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
    
    <!-- D3.js for visualizations -->
    <script src="https://d3js.org/d3.v5.min.js"></script>
</head>
<body>
    <div class="main-container">
        <!-- Top Input Section -->
        <div class="input-section">
            <h1>🧠 Graph Attention Networks (GAT) Explorer</h1>
            <div class="paragraph-input">
                <label for="paragraph-input">Enter Paragraph for GAT Processing:</label>
                <textarea id="paragraph-input" placeholder="Enter multiple sentences here for complete GAT analysis..." rows="3">Neural networks learn patterns efficiently. Graph attention mechanisms capture relationships. These models transform natural language processing.</textarea>
                <div class="controls">
                    <button id="process-gat" onclick="processWithGAT()">🚀 Process with GAT</button>
                    <button id="step-through" onclick="stepThroughProcess()">📝 Step-by-Step</button>
                    <select id="stage-selector">
                        <option value="tokenization">1. Tokenization</option>
                        <option value="embedding">2. Embeddings</option>
                        <option value="attention">3. Attention Computation</option>
                        <option value="aggregation">4. Message Aggregation</option>
                        <option value="final">5. Final Graph</option>
                    </select>
                </div>
            </div>
        </div>

        <!-- Main Content Area -->
        <div class="content-container">
            <!-- Left Panel: ASCII Explanations -->
            <div class="explanation-panel">
                <h2>📚 Step-by-Step Process</h2>
                
                <div id="step-1" class="explanation-step active">
                    <h3>🔤 Step 1: Tokenization</h3>
                    <pre class="ascii-diagram">
INPUT: "Neural networks learn patterns efficiently"
       ↓ (word-level splitting)
TOKENS: ["Neural", "networks", "learn", "patterns", "efficiently"]
        ↓ (index assignment)  
INDICES: [0, 1, 2, 3, 4]

┌─────────────────────────────────────────────┐
│          TOKEN TO INDEX MAPPING             │
├─────────────────────────────────────────────┤
│ Neural     → 0                              │
│ networks   → 1                              │
│ learn      → 2                              │
│ patterns   → 3                              │
│ efficiently→ 4                              │
└─────────────────────────────────────────────┘
                    </pre>
                </div>

                <div id="step-2" class="explanation-step">
                    <h3>🎯 Step 2: Embedding Lookup</h3>
                    <pre class="ascii-diagram">
TOKEN EMBEDDINGS (256-dimensional vectors):

Neural     → [0.2, -0.1, 0.8, ..., 0.3]
networks   → [0.5, 0.2, -0.3, ..., 0.1] 
learn      → [-0.1, 0.7, 0.4, ..., -0.2]
patterns   → [0.3, -0.5, 0.1, ..., 0.8]
efficiently→ [0.1, 0.4, -0.6, ..., 0.2]

┌─────────────────────────────────────────────┐
│           EMBEDDING MATRIX H                │
├─────────────────────────────────────────────┤
│        dim₀   dim₁   dim₂  ...  dim₂₅₅      │
│ tok₀ │  0.2  -0.1   0.8  ...   0.3  │       │
│ tok₁ │  0.5   0.2  -0.3  ...   0.1  │       │
│ tok₂ │ -0.1   0.7   0.4  ...  -0.2  │       │
│ tok₃ │  0.3  -0.5   0.1  ...   0.8  │       │
│ tok₄ │  0.1   0.4  -0.6  ...   0.2  │       │
└─────────────────────────────────────────────┘
                    </pre>
                </div>

                <div id="step-3" class="explanation-step">
                    <h3>⚡ Step 3: Attention Computation</h3>
                    <pre class="ascii-diagram">
ATTENTION MECHANISM:
For each pair (i,j):

┌─────────────────────────────────────────────┐
│ e_ij = LeakyReLU(a^T [W·h_i || W·h_j])      │
│                    ↑_____________↑          │
│                 transformed   transformed   │
│                 features      features      │
└─────────────────────────────────────────────┘

ATTENTION SCORES (before normalization):
     0     1     2     3     4
  ┌─────────────────────────────┐
0 │ 0.0   2.1   0.8   0.3   0.1 │
1 │ 1.8   0.0   2.5   0.7   0.2 │  
2 │ 0.5   1.9   0.0   2.3   0.6 │
3 │ 0.2   0.8   1.7   0.0   2.0 │
4 │ 0.1   0.3   0.9   1.5   0.0 │
  └─────────────────────────────┘

NORMALIZED ATTENTION (softmax):
     0     1     2     3     4
  ┌─────────────────────────────┐
0 │ 0.00  0.65  0.25  0.07  0.03│ ← Row sums to 1.0
1 │ 0.15  0.00  0.70  0.12  0.03│
2 │ 0.08  0.32  0.00  0.55  0.05│
3 │ 0.03  0.12  0.30  0.00  0.55│
4 │ 0.02  0.06  0.22  0.40  0.00│
  └─────────────────────────────┘
                    </pre>
                </div>

                <div id="step-4" class="explanation-step">
                    <h3>🔄 Step 4: Message Aggregation</h3>
                    <pre class="ascii-diagram">
MESSAGE PASSING:
h'_i = σ(Σⱼ α_ij · W · hⱼ)

For token "learn" (index 2):
┌─────────────────────────────────────────────┐
│ h'₂ = σ(α₂₀·W·h₀ + α₂₁·W·h₁ + α₂₂·W·h₂ +    │
│          α₂₃·W·h₃ + α₂₄·W·h₄)               │
│                                             │
│     = σ(0.08·W·h₀ + 0.32·W·h₁ + 0.00·W·h₂ + │
│         0.55·W·h₃ + 0.05·W·h₄)              │
│                                             │
│ "learn" pays most attention to "patterns"   │
│ (weight 0.55) and "networks" (weight 0.32)  │
└─────────────────────────────────────────────┘

AGGREGATED FEATURES:
Neural     → h'₀ = [0.35, 0.12, -0.1, ..., 0.8]
networks   → h'₁ = [0.28, -0.05, 0.6, ..., 0.2]
learn      → h'₂ = [0.15, 0.45, 0.3, ..., -0.1]
patterns   → h'₃ = [0.42, -0.2, 0.1, ..., 0.7]
efficiently→ h'₄ = [0.18, 0.3, -0.4, ..., 0.1]
                    </pre>
                </div>

                <div id="step-5" class="explanation-step">
                    <h3>🎯 Step 5: Final Graph Representation</h3>
                    <pre class="ascii-diagram">
FINAL ATTENTION GRAPH:
         ╭─0.25─╮     ╭─0.30─╮
      ╭─────╮   │  ╭─────╮   │
  ┌─────────┐   │  │ ┌─────────┐   │  ┌─────────┐
  │ Neural  │───┼──┼→│networks │───┼──┼→│  learn  │
  │   (0)   │   │  │ │   (1)   │   │  │ │   (2)   │
  └─────────┘   │  │ └─────────┘   │  │ └─────────┘
      ↑         │  │     ↑         │  │     ↑
      ╰─0.15────╯  │     ╰─0.32────╯  │     ╰─0.55──╮
                   ╰─0.65─────────────╯              │
                                                     ↓
                                              ┌─────────┐
                                              │patterns │
                                              │   (3)   │
                                              └─────────┘

DENSE ADJACENCY MATRIX:
All connections preserved with learned weights
Used for downstream tasks (classification, etc.)
                    </pre>
                </div>
            </div>

            <!-- Right Panel: Live Visualizations -->
            <div class="visualization-panel">
                <h2>🎨 Live Visualizations</h2>
                
                <div id="stage-visualization">
                    <!-- Current Stage Display -->
                    <div class="stage-header">
                        <h3 id="current-stage">Stage 1: Tokenization</h3>
                        <div class="stage-progress">
                            <div class="progress-bar">
                                <div id="progress-fill" style="width: 20%"></div>
                            </div>
                            <span id="progress-text">1/5 Steps</span>
                        </div>
                    </div>

                    <!-- Tokenization Visualization -->
                    <div id="tokenization-viz" class="stage-viz active">
                        <div class="tokens-display">
                            <div class="original-text">
                                <span class="label">Original Text:</span>
                                <div id="original-text-display"></div>
                            </div>
                            <div class="arrow-down">↓</div>
                            <div class="tokens-list">
                                <span class="label">Tokens:</span>
                                <div id="tokens-display"></div>
                            </div>
                        </div>
                    </div>

                    <!-- Embedding Visualization -->
                    <div id="embedding-viz" class="stage-viz">
                        <div class="embeddings-display">
                            <canvas id="embedding-canvas" width="400" height="300"></canvas>
                            <div class="embedding-info">
                                <span class="label">Embedding Dimensions: 256</span>
                                <div id="embedding-stats"></div>
                            </div>
                        </div>
                    </div>

                    <!-- Attention Visualization -->
                    <div id="attention-viz" class="stage-viz">
                        <div class="attention-display">
                            <div class="attention-matrix">
                                <canvas id="attention-canvas" width="400" height="400"></canvas>
                            </div>
                            <div class="attention-info">
                                <span class="label">Attention Weights</span>
                                <div id="attention-stats"></div>
                            </div>
                        </div>
                    </div>

                    <!-- Aggregation Visualization -->
                    <div id="aggregation-viz" class="stage-viz">
                        <div class="message-passing">
                            <canvas id="aggregation-canvas" width="400" height="300"></canvas>
                            <div class="aggregation-info">
                                <span class="label">Message Aggregation</span>
                                <div id="aggregation-stats"></div>
                            </div>
                        </div>
                    </div>

                    <!-- Final Graph Visualization -->
                    <div id="final-viz" class="stage-viz">
                        <div id="final-graph-container">
                            <!-- Our existing text-as-graph visualization will go here -->
                            <div id='text-as-graph-final'></div>
                        </div>
                        <div class="comparison-view">
                            <div class="sequential-graph">
                                <h4>Sequential Graph (Current)</h4>
                                <div id="sequential-visualization"></div>
                            </div>
                            <div class="attention-graph">
                                <h4>GAT Graph (With Attention)</h4>
                                <div id="attention-graph-visualization"></div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Controls -->
                <div class="visualization-controls">
                    <button id="prev-step" onclick="previousStep()">← Previous</button>
                    <button id="next-step" onclick="nextStep()">Next →</button>
                    <button id="auto-play" onclick="autoPlay()">🎬 Auto Play</button>
                    <div class="speed-control">
                        <label>Speed:</label>
                        <input type="range" id="speed-slider" min="500" max="3000" value="1500">
                        <span id="speed-display">1.5s</span>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="gat-explorer.js"></script>
</body>
</html>